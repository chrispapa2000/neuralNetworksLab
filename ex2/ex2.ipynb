{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ομάδα 70:\n",
    "**Χρήστος Παπαδημητρίου, el18017**\n",
    "\n",
    "**Ζαχαρίας-Παύλος Αναστασιάδης, el18161**\n",
    "\n",
    "**Βαρθολομαίος Βαμβακάρης, el18071**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLFXD3IePSyL"
   },
   "source": [
    "# Εργαστηριακή Άσκηση 2. Μη επιβλεπόμενη μάθηση. \n",
    "## Σύστημα συστάσεων βασισμένο στο περιεχόμενο\n",
    "## Σημασιολογική απεικόνιση δεδομένων με χρήση SOM \n",
    "Ημερομηνία εκφώνησης της άσκησης: 29 Νοεμβρίου 2022\n",
    "\n",
    "**Θα βρείτε το παρόν σε μορφή jupyter notebook ως συνημμένο στο τέλος της εκφώνησης.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "S5wbBzIYnird"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n",
      "^C\n",
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "!conda install numpy\n",
    "!conda install pandas\n",
    "!conda install nltk\n",
    "!conda install scikit-learn\n",
    "!conda install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aViHqlQcPSyP"
   },
   "source": [
    "## Εισαγωγή του Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZVmdDExPSyQ"
   },
   "source": [
    "Το σύνολο δεδομένων με το οποίο θα δουλέψουμε είναι βασισμένο στο [Carnegie Mellon Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/). Πρόκειται για ένα dataset με 22.301 περιγραφές ταινιών. Η περιγραφή κάθε ταινίας αποτελείται από τον τίτλο της, μια ή περισσότερες ετικέτες που χαρακτηρίζουν το είδος της ταινίας και τέλος τη σύνοψη της υπόθεσής της. Αρχικά εισάγουμε το dataset (χρησιμοποιήστε αυτούσιο τον κώδικα, δεν χρειάζεστε το αρχείο csv) στο dataframe `df_data_1`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "62SOj46gPSyS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_url = \"https://drive.google.com/uc?export=download&id=1zo13kUAf-MDMPZmBDxq1FxWtZY01lsxD\"\n",
    "df_data_1 = pd.read_csv(dataset_url, sep='\\t',  header=None, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TAEZGdIPSyW"
   },
   "source": [
    "Κάθε ομάδα θα δουλέψει σε **ένα μοναδικό υποσύνολο 5.000 ταινιών** (διαφορετικό dataset για κάθε ομάδα) ως εξής:\n",
    "\n",
    "1. Κάθε ομάδα του εργαστηρίου νευρωνικών έχει έναν αριθμό στο helios. Θα βάλετε τον αριθμό αυτό στη μεταβλητή team_seed_number στο επόμενο κελί κώδικα.\n",
    "\n",
    "2. Το data frame `df_data_2` έχει γραμμές όσες και οι ομάδες και 5.000 στήλες. Σε κάθε ομάδα αντιστοιχεί η γραμμή του πίνακα με το `team_seed_number` της. Η γραμμή αυτή θα περιλαμβάνει 5.000 διαφορετικούς αριθμούς που αντιστοιχούν σε ταινίες του αρχικού dataset. \n",
    "\n",
    "3. Τρέξτε τον κώδικα. Θα προκύψουν τα μοναδικά για κάθε ομάδα  titles, categories, catbins, summaries και corpus με τα οποία θα δουλέψετε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2POlqDjkPSyY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Στο επόμενη γραμή βάλτε τον αριθμό της ομάδας στο εργαστήριο των νευρωνικών\n",
    "team_seed_number = 70\n",
    "\n",
    "movie_seeds_url = \"https://drive.google.com/uc?export=download&id=1g6F4TCHrs2wgtdOk7D3gtONaeirNt_Vo\"\n",
    "df_data_2 = pd.read_csv(movie_seeds_url, header=None)\n",
    "\n",
    "# επιλέγεται \n",
    "my_index = df_data_2.iloc[team_seed_number,:].values\n",
    "\n",
    "titles = df_data_1.iloc[:, [2]].values[my_index] # movie titles (string)\n",
    "categories = df_data_1.iloc[:, [3]].values[my_index] # movie categories (string)\n",
    "bins = df_data_1.iloc[:, [4]]\n",
    "catbins = bins[4].str.split(',', expand=True).values.astype(float)[my_index] # movie categories in binary form (1 feature per category)\n",
    "summaries =  df_data_1.iloc[:, [5]].values[my_index] # movie summaries (string)\n",
    "corpus = summaries[:,0].tolist() # list form of summaries\n",
    "corpus_df = pd.DataFrame(corpus) # dataframe version of corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If66lkwxPSyb"
   },
   "source": [
    "- Ο πίνακας **titles** περιέχει τους τίτλους των ταινιών. Παράδειγμα: 'Sid and Nancy'.\n",
    "- O πίνακας **categories** περιέχει τις κατηγορίες (είδη) της ταινίας υπό τη μορφή string. Παράδειγμα: '\"Tragedy\",  \"Indie\",  \"Punk rock\",  \"Addiction Drama\",  \"Cult\",  \"Musical\",  \"Drama\",  \"Biopic \\[feature\\]\",  \"Romantic drama\",  \"Romance Film\",  \"Biographical film\"'. Παρατηρούμε ότι είναι μια comma separated λίστα strings, με κάθε string να είναι μια κατηγορία.\n",
    "- Ο πίνακας **catbins** περιλαμβάνει πάλι τις κατηγορίες των ταινιών αλλά σε δυαδική μορφή ([one hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)). Έχει διαστάσεις 5.000 x 322 (όσες οι διαφορετικές κατηγορίες). Αν η ταινία ανήκει στο συγκεκριμένο είδος η αντίστοιχη στήλη παίρνει την τιμή 1, αλλιώς παίρνει την τιμή 0.\n",
    "- Ο πίνακας **summaries** και η λίστα **corpus** περιλαμβάνουν τις συνόψεις των ταινιών (η corpus είναι απλά ο summaries σε μορφή λίστας). Κάθε σύνοψη είναι ένα (συνήθως μεγάλο) string. Παράδειγμα: *'The film is based on the real story of a Soviet Internal Troops soldier who killed his entire unit  as a result of Dedovschina. The plot unfolds mostly on board of the prisoner transport rail car guarded by a unit of paramilitary conscripts.'*\n",
    "- το dataframe **corpus_df** που είναι απλά το corpus σε μορφή dataframe. Τα summaries βρίσκονται στην κολόνα 0. Πιθανώς να σας βολεύει να κάνετε κάποιες προεπεξεργασίες με dataframes.\n",
    "\n",
    "\n",
    "Θεωρούμε ως **ID** της κάθε ταινίας τον αριθμό γραμμής της ή το αντίστοιχο στοιχείο της λίστας. Παράδειγμα: για να τυπώσουμε τη σύνοψη της ταινίας με `ID=999` (την χιλιοστή) θα γράψουμε `print(corpus[999])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k_7A3KXLp0qS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eddelu Manjunatha']\n",
      "['\"Drama\",  \"Comedy\"']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "This film is about the life and times of a lazy and jobless middle aged man named Manjunatha  who considers his life as an eventual existence rather than a practical, deserving and a capable one. His laziness is portrayed in the film as not a quality but as an ethic imbibed within his general thoughts and notions of the everyday world that surrounds him. Through his formative years his thinking becomes pragmatic in considering that livelihood can always be sought through alternate sources rather than being a puppet to how the world goes about through linear methods of gaining success and money. The other essentially important character in the film is a visually impaired person named Naani . Although being born blind, he wishes to be a film director. Naani's most positive aspect of the role is that he does not believe that his physical disability could stop him from achieving what he dreams, that is to be a film maker. The film is structured around the lengthy conversation, which happen in a captive lodge room, between Manja and Naani where each depict their ideologies and experiences and co-relate their thoughts. Being a Jaggesh film, the film is no where short in containing blatant satirical humour and constant metaphors. The characters of Manja and Naani form contrasting personalities - Manja being an unmotivated, lazy and irresponsible guy and Naani being optimistic and ambitious. Naani happens to meet Manja in a very interesting scenario. Even with their contrasting personalities, Manja and Naani get along together pretty well. The other important, yet minimally portrayed, role is of Manja's wife Gowri . Manja admittedly marries Gowri in the fact that he was getting a small house as a dowry. Gowri struggles to save her relationship while Manja is doused into the casual habits of alcohol, his influential \"circle\" of friends, betting, occasional petty thieving at random jobs, inability to sustain decent jobs, wife-bitching and other habits including schemes that eventually thicken the gap between living a moral and a meaningful life and being an incapable disloyal husband. However Gowri's character is portrayed to be of a devout woman who honours the capacity of developing a more healthy family hood through her husband changing his ways some day or the other. But days and years go on and Manja's lifestyle remains unchanged much to the chagrin of Gowri. The conversation now continues shifting from the lodge to Manja's home itself. After celebrating their freedom from the lodge with alcoholism that night they find themselves again captive within the house due to the help of the local inspector who assists Gowri to tackle Manja's unyielding ways. Naani then talks about the plot of his film, which was seemingly ignored by the producer with whom he had placed his trust upon . Manja, hearing of the simple story of Dr. Rajkumar's pledged eyes and how they were now seeing a world through another person, is taken aback and applauds Naani for such a heartwarming plot and how the -Annavru-'s fans would welcome such a movie. He motivates Naani with all success if Naani ever made the film by taking out his mother's prized 50 rupee note from the cupboard and giving it to Naani. He tells Naani that it was considered as a luck charm to any person that received it. He also happens to find a note in the cupboard that Gowri leaves behind  to Manja conveying that she was now carrying and that he would soon become a father. Manja's personality suddenly defines a change after reading the news. He is unable to express his joy, apart from sharing it with Naani, at that moment being locked in his home. He tries calling Gowri but he doesn't get her on-line. In the midst of all this there concocts a life turning situation for Manja at that moment. His wife returns home struck in pain. Gowri had killed the developing child in her womb due to the burdensome worry which she concluded that she wasn't in a state to be able to maintain and grow a child while having such a lackluster and incapable husband. She perceived that it was best for the child to not come to life and face a deteriorated lifestyle. Naani leaves the house expressing his ill timed presence in the development of such an event. Manja is clipped between a moment of where he faced fresh joy like he had not known for a long long time where he believed that the child, who would be his Lakshmi , would change his life for the better and to another moment that his 'Lakshmi' would not be happening. In his state of hopelessness he threatens Gowri as to what rights she held to kill his child. Gowri is throbbing in pain to be able to reply to his questions. In this delusion of Manja, Lakshmi -the unborn child, appears to him and speaks to him as to how ill fated she would have been to have been born as a daughter to such a father. Lakshmi says that Gowri, her mother, did not kill her and that Manja, her father, killed her and suggests to him that he could celebrate this occasion with his friends by drinking along with them. Manja's remorse knows no bounds. He reflects back Lakshmi's words to Gowri and says to his wife that neither his own parents or his own wife or any of his gurus could ever be a guru to him, but the unborn dead child which will never happen in his existence was his ultimate guru to his final immediate realization of the value of life.\n"
     ]
    }
   ],
   "source": [
    "ID = 999\n",
    "print(titles[ID])\n",
    "print(categories[ID])\n",
    "print(catbins[ID])\n",
    "print(corpus[ID])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTNgwBfjPSyc"
   },
   "source": [
    "# Εφαρμογή 1. Υλοποίηση συστήματος συστάσεων ταινιών βασισμένο στο περιεχόμενο\n",
    "<img src=\"http://clture.org/wp-content/uploads/2015/12/Netflix-Streaming-End-of-Year-Posts.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnA2RP8GPSyf"
   },
   "source": [
    "Η πρώτη εφαρμογή που θα αναπτύξετε θα είναι ένα [σύστημα συστάσεων](https://en.wikipedia.org/wiki/Recommender_system) ταινιών βασισμένο στο περιεχόμενο (content based recommender system). Τα συστήματα συστάσεων στοχεύουν στο να προτείνουν αυτόματα στο χρήστη αντικείμενα από μια συλλογή τα οποία ιδανικά θέλουμε να βρει ενδιαφέροντα ο χρήστης. Η κατηγοριοποίηση των συστημάτων συστάσεων βασίζεται στο πώς γίνεται η επιλογή (filtering) των συστηνόμενων αντικειμένων. Οι δύο κύριες κατηγορίες είναι η συνεργατική διήθηση (collaborative filtering) όπου το σύστημα προτείνει στο χρήστη αντικείμενα που έχουν αξιολογηθεί θετικά από χρήστες που έχουν παρόμοιο με αυτόν ιστορικό αξιολογήσεων και η διήθηση με βάση το περιεχόμενο (content based filtering), όπου προτείνονται στο χρήστη αντικείμενα με παρόμοιο περιεχόμενο (με βάση κάποια χαρακτηριστικά) με αυτά που έχει προηγουμένως αξιολογήσει θετικά.\n",
    "\n",
    "Το σύστημα συστάσεων που θα αναπτύξετε θα βασίζεται στο **περιεχόμενο** και συγκεκριμένα στις συνόψεις των ταινιών (corpus). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3nZv-xueEtd"
   },
   "source": [
    "## Προεπεξεργασία\n",
    "\n",
    "Το πρώτο βήμα στην επεξεργασία μας είναι ο καθαρισμός των περιγραφών των ταινιών. \n",
    "\n",
    "Εκτυπώστε (αρκετές) διαφορετικές περιγραφές ταινιών για να δείτε πιθανά προβλήματα που θα πρέπει να αντιμετωπιστούν.\n",
    "\n",
    "Τα (ελάχιστα) βήματα καθαρισμού που προτείνουμε είναι:\n",
    "- μετατροπή όλων των χαρακτήρων σε πεζά,\n",
    "- αφαίρεση των stopwords. Εδώ σημειώστε ότι για το δεδομένο task του συστήματος συστάσεων που είναι η πρόταση ταινιών ίσως θα είχαν ενδιαφέρον και λίστες stopwords πέραν αυτών της κοινής γλώσσας.\n",
    "- αφαίρεση σημείων στίξης και ειδικών χαρακτρήρων (special characters). Αυτό δεν γίνεται μόνο με την punkt του NLTK. Θα μπορούσατε να βασιστείτε σε κανονικές εκφράσεις (regular expressions), και\n",
    "- αφαίρεση πολυ σύντομων συμβολοσειρών.\n",
    "\n",
    "Προσοχή: το corpus και τα τελικά tokens που θα το αποτελούν θα χρησιμοποιηθούν στη συνέχεια ως κλειδιά για να βρούμε εμφυτεύματα. Για το λόγο αυτό, πρέπει να είστε προσεκτικοί ως προς την εφαρμογή μεθόδων κανονικοποίησης (text normalization) όπως το stemming και το lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Μετατροπή όλων των χαρακτήρων σε πεζα**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all2lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(map(all2lowercase, corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Αφαίρεση Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Αφαίρεση Σημείων στίξης**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkExtraChars(word, chars):\n",
    "    l = []\n",
    "    final_word = \"\"\n",
    "    for letter in word:\n",
    "        l.append((letter in chars))\n",
    "        final_word = final_word+letter if letter not in chars else final_word\n",
    "    return final_word if not all(l) else None        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Αφαίρεση πολύ σύντομων συμβολοσειρών**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(document):\n",
    "    # all words to lowercase\n",
    "    words = nltk.word_tokenize(document.lower())\n",
    "\n",
    "    # remove stopwords\n",
    "    myStopWords = [\"film\", \"movie\", \"story\", \"n't\"]\n",
    "    words_no_stopwords = [word for word in words if word not in stopwords.words('english')+myStopWords]\n",
    "    \n",
    "    # remove punctuation\n",
    "    words_no_punctuation = [word for word in words_no_stopwords if word not in list(string.punctuation)]\n",
    "    \n",
    "    # replace some strings\n",
    "    words_replaced =  list(map(lambda word: word.replace('.', ''), words_no_punctuation))\n",
    "    \n",
    "    # remove some special characters\n",
    "    words_no_extra_chars, extraChars = [], [\"'\", \"`\", \"\\\"\", \"-\", \"_\"]\n",
    "    for word in words_replaced:\n",
    "        if checkExtraChars(word, extraChars) != None:\n",
    "            words_no_extra_chars.append(checkExtraChars(word, extraChars))\n",
    "            \n",
    "    # remove urls (which we identify as strings with more than one '/')\n",
    "    words_no_urls = []\n",
    "    for word in words_no_extra_chars:\n",
    "        if word.count('/') < 2:\n",
    "            words_no_urls.append(word.replace('/', ''))            \n",
    "            \n",
    "    # remove strings containing some special characters ('|', '=', '+')\n",
    "    words_no_special_chars = []\n",
    "    for word in words_no_urls:\n",
    "        if not ('|' in word or '+' in word or '=' in word):\n",
    "            words_no_special_chars.append(word)   \n",
    "\n",
    "    \n",
    "    # remove string with size 1\n",
    "    words_no_small = [word for word in words_no_special_chars if len(word)>1]\n",
    "    \n",
    "    return words_no_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "preprocessed_corpus = [preprocess_document(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert corpus to a list of strings\n",
    "preprocessed_corpus_string = list(map(lambda doc : ' '.join(doc) , preprocessed_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD5KuSKrxQ8I"
   },
   "source": [
    "## Μετατροπή σε TFIDF\n",
    "\n",
    "Το πρώτο βήμα θα είναι λοιπόν να μετατρέψετε το corpus σε αναπαράσταση tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "s5YP6XCZPSyh"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create sparse tf_idf representation\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(preprocessed_corpus_string)\n",
    "corpus_tf_idf_plain = vectorizer.transform(preprocessed_corpus_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-uRZK3EPSyl"
   },
   "source": [
    "Η συνάρτηση [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) όπως καλείται εδώ **δεν είναι βελτιστοποιημένη**. Οι επιλογές των μεθόδων και παραμέτρων της μπορεί να έχουν **δραματική επίδραση στην ποιότητα των συστάσεων** και είναι διαφορετικές για κάθε dataset. Επίσης, οι επιλογές αυτές έχουν πολύ μεγάλη επίδραση και στη **διαστατικότητα και όγκο των δεδομένων**. Η διαστατικότητα των δεδομένων με τη σειρά της θα έχει πολύ μεγάλη επίδραση στους **χρόνους εκπαίδευσης**, ιδιαίτερα στη δεύτερη εφαρμογή της άσκησης.\n",
    "\n",
    "Προσοχή: ο TfidfVectorizer έχει κάποιες δυνατότητες προεπεξεργασίας παρόποιες με αυτές που αναφέραμε στην προηγούμενη ενότητα. Ό,τι προεπεξεργασία μπορείτε να κάνετε που χρειάζεται ως είσοδο μόνο το κάθε document ξεχωριστά, κάντε την στο πρώτο βήμα της προεπεξεργασίας. Αν χρειάζεται γνώση των συνολικών στατιστικών της συλλογής, κάντε την με τον TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "y_Cw0brpnisF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 51660)\n"
     ]
    }
   ],
   "source": [
    "print(corpus_tf_idf_plain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"marie skinner  is a gold digger with her hooks out for devoted middle-aged family man j.c. hudson , a portly real estate tycoon, who falls for her when she contrives to meet him. when his wife  and grown children, ruth  and billy  discover him dancing with marie at a nightclub, j.c. leaves home the next day. ruth seeks out marie to shoot her, but is interrupted by marie's boyfriend, jazz hound babe winsor , who takes a shine to her. when judson walks in on them he condemns her licentiousness, but is forced to face his double standard when he witnesses a violent argument between marie and babe. full of contrition, j.c. returns to home and hearth and the bosom of his loving family.jhailey@hotmail.com plot summarywollstein, hans j. plot synopsistcm synopsis full synopsis\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf = pd.DataFrame(corpus_tf_idf_plain.toarray(), columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LsmvSyVykTU"
   },
   "source": [
    "## Υλοποίηση του συστήματος συστάσεων\n",
    "\n",
    "Το σύστημα συστάσεων που θα υλοποιήσετε θα είναι μια συνάρτηση `content_recommender` με τρία ορίσματα: `target_movie`, `max_recommendations` και `corpus_type`. Στην `target_movie` περνάμε το ID μιας ταινίας-στόχου για την οποία μας ενδιαφέρει να βρούμε παρόμοιες ως προς το περιεχόμενο (τη σύνοψη) ταινίες, `max_recommendations` στο πλήθος.\n",
    "Υλοποιήστε τη συνάρτηση ως εξής: \n",
    "- για την ταινία-στόχο, θα υπολογίζετε την [ομοιότητα συνημιτόνου](https://en.wikipedia.org/wiki/Cosine_similarity) της με όλες τις ταινίες της συλλογής σας όπως αυτές αναπαριστώνται στο `corpus_type`.\n",
    "- με βάση την ομοιότητα συνημιτόνου που υπολογίσατε, δημιουργήστε ταξινομημένο πίνακα από το μεγαλύτερο στο μικρότερο, με τα indices (`ID`) των ταινιών. Παράδειγμα: αν η ταινία με index 1 έχει ομοιότητα συνημιτόνου με 3 ταινίες \\[0.2 1 0.6\\] (έχει ομοιότητα 1 με τον εαύτό της) ο ταξινομημένος αυτός πίνακας indices θα είναι \\[1 2 0\\].\n",
    "- Για την ταινία-στόχο εκτυπώστε: id, τίτλο, σύνοψη, κατηγορίες (categories)\n",
    "- Για τις `max_recommendations` ταινίες (πλην της ίδιας της ταινίας-στόχου που έχει cosine similarity 1 με τον εαυτό της) με τη μεγαλύτερη ομοιότητα συνημιτόνου (σε φθίνουσα σειρά), τυπώστε σειρά σύστασης (1 πιο κοντινή, 2 η δεύτερη πιο κοντινή κλπ), ομοιότητα συνημιτόνου, id, τίτλο, σύνοψη, και κατηγορίες (categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "def createRow(ind,extra=''):\n",
    "    return \"|\" + str(ind)+extra + \"|\" + titles[ind][0]+ \"|\" + str(corpus[ind]) + \"|\" + str(categories[ind]) +\"|\\n\"\n",
    "\n",
    "def content_recommender(target_movie, max_recommendations, corpus_type):\n",
    "    (height, width) = corpus_type.shape\n",
    "    distances = np.zeros(height)\n",
    "    for i in range(height):\n",
    "        distances[i] = sp.spatial.distance.cosine(corpus_type[target_movie].toarray()[0],\n",
    "                                                  corpus_type[i].toarray()[0])\n",
    "        \n",
    "    order = sorted(range(len(distances)), key = (lambda length : distances[length]))\n",
    "    order.remove(target_movie)\n",
    "    \n",
    "    disp = \"|Movie|Title|description|categories|\\n|:-:|:-:|:-:|:-:|\\n\"\n",
    "    disp += createRow(target_movie,extra=\"(target)\")\n",
    "    for i in range(0,max_recommendations):\n",
    "        disp += createRow(order[i])\n",
    "    display(Markdown(disp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|Movie|Title|description|categories|\n",
       "|:-:|:-:|:-:|:-:|\n",
       "|2259(target)|Signs of Life|during world war ii, three german soldiers are withdrawn from combat when one of them, stroszek, is wounded. they are assigned to a small coastal community on the greek island of kos while stroszek recuperates. the men become increasingly stir crazy in their uneventful new assignment. stroszek eventually goes mad.|['\"Drama\",  \"World cinema\"']|\n",
       "|3791|The Crossing of the Rhine|the film tells the story of two french soldiers in the aftermath of the german invasion of france who become forced labourers on a german farm under the service du travail obligatoire programme , but become involved in the lives of their captors.|['\"Drama\",  \"War film\"']|\n",
       "|4193|Tokyo Park|a college student, aspiring to become a photographer, receives a job to follow a client's girlfriend. this assignment changes his relationship with women around him.|['\"Japanese Movies\"']|\n",
       "|1796|Breaker Morant|three australian army officers of the bushveldt carbineers serving in south africa during the second boer war  are on trial for murder. lieutenants harry \"breaker\" morant, peter handcock, and george witton are accused of the murder of one boer prisoner and the subsequent murders of six more. in addition, morant and handcock are accused of the sniper-style assassination of a german missionary, the rev. h. c. v. hesse. their defence counsel, major j. f. thomas, has had only one day to prepare their defence. lord kitchener, who ordered the trial, hopes to bring the boer war to an end with a peace conference. to that end, he uses the morant trial to show that he is willing to judge his own soldiers harshly if they disobey the rules of war. although, as major thomas mentions in court, there are great complexities associated with charging active-duty soldiers with murder during battle, kitchener is determined to have a guilty verdict, and the chief of the court, lt. colonel denny, supports him. major thomas' speech on the \"barbarities of war\" provides the climax of the film: now, when the rules and customs of war are departed from by one side, one must expect the same sort of behaviour from the other. accordingly, officers of the carbineers should be, and up until now have been, given the widest possible discretion in their treatment of the enemy.now, i don't ask for proclamations condoning distasteful methods of war, but i do say that we must take for granted that it does happen. let's not give our officers hazy, vague instructions about what they may or may not do. let's not reprimand them, on the one hand for hampering the column with prisoners, and at another time and another place, hold them up as murderers for obeying orders.[...]the fact of the matter is that war changes men's natures. the barbarities of war are seldom committed by abnormal men. the tragedy of war is that these horrors are committed by normal men in abnormal situations, situations in which the ebb and flow of everyday life have departed and have been replaced by a constant round of fear, and anger, blood, and death. soldiers at war are not to be judged by civilian rules, as the prosecution is attempting to do, even though they commit acts which, calmly viewed afterwards, could only be seen as unchristian and brutal. and if, in every war, particularly guerilla war, all the men who committed reprisals were to be charged and tried as murderers, court martials like this one would be in permanent session. would they not?i say that we cannot hope to judge such matters unless we ourselves have been submitted to the same pressures, the same provocations as these men, whose actions are on trial.|['\"Anti-war\",  \"British Empire Film\",  \"History\",  \"Period piece\",  \"Drama\",  \"War film\",  \"Anti-war film\",  \"Courtroom Drama\"']|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "r = random.randint(1,5000)\n",
    "content_recommender(2259,3,corpus_tf_idf_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IvHkTUHyu78"
   },
   "source": [
    "## Βελτιστοποίηση του TfidfVectorizer\n",
    "\n",
    "Αφού υλοποιήσετε τη συνάρτηση `content_recommender` χρησιμοποιήστε την για να βελτιστοποιήσετε την `TfidfVectorizer`. Συγκεκριμένα, αρχικά μπορείτε να δείτε τι επιστρέφει το σύστημα για τυχαίες ταινίες-στόχους και για ένα μικρό `max_recommendations` (2 ή 3). Αν σε κάποιες ταινίες το σύστημα μοιάζει να επιστρέφει σημασιολογικά κοντινές ταινίες σημειώστε το `ID` τους. Δοκιμάστε στη συνέχεια να βελτιστοποιήσετε την `TfidfVectorizer` για τα συγκεκριμένα `ID` ώστε να επιστρέφονται σημασιολογικά κοντινές ταινίες για μεγαλύτερο αριθμό `max_recommendations`. Παράλληλα, όσο βελτιστοποιείτε την `TfidfVectorizer`, θα πρέπει να λαμβάνετε καλές συστάσεις για μεγαλύτερο αριθμό τυχαίων ταινιών. \n",
    "\n",
    "Ταυτόχρονα, μια αντίρροπη κατά κάποιο τρόπο κατεύθυνση της βελτιστοποίησης είναι να χρησιμοποιείτε τις παραμέτρους του `TfidfVectorizer` έτσι ώστε να μειώνονται οι διαστάσεις του Vector Space Model μέχρι το σημείο που θα αρχίσει να εμφανίζονται επιπτώσεις στην ποιότητα των συστάσεων. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ids: 1580, 3784, 2359, 2259, 1448, 3786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse tf_idf representation\n",
    "vectorizer_improved = TfidfVectorizer(max_df=0.95, min_df=0.05)\n",
    "vectorizer_improved.fit(preprocessed_corpus_string)\n",
    "corpus_tf_idf_improved = vectorizer_improved.transform(preprocessed_corpus_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 357)\n"
     ]
    }
   ],
   "source": [
    "print(corpus_tf_idf_improved.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSQ2tCs_hbaH"
   },
   "source": [
    "## Βαθιά μάθηση: δημιουργία corpora με χρήση word emmbeddings\n",
    "\n",
    "Η προσέγγιση της κατασκευής μόνο μέσω tfidf του συστήματος συστάσεων έχει διάφορα μειονεκτήματα. Θα μας ενδιέφερε λοιπόν να δούμε αν μπορούμε να χρησιμοποιήσουμε για τις λέξεις **εμφυτεύματα (embeddings)**, δηλαδή τις πυκνές διανυσματικές αναπαραστάσεις για τις λέξεις που μας δίνει το μοντέλο **Word2Vec**\n",
    "\n",
    "Ωστόσο, το dataset της κάθε ομάδας είναι πολύ μικρό για να εξάγουμε τα δικά μας word embeddings (και να είναι καλά). Για το λόγο αυτό θα χρησιμοποιήσουμε τη μεθοδολογία της Βαθιάς Μάθησης που είναι η **Μεταφορά Μάθησης (Transfer Learning).**.\n",
    "\n",
    "Στη μεταφορά μάθησης ουσιαστικά μεταφέρουμε τη γνώση που έχει αποκτήσει ένα ήδη εκπαιδευμένο (και κατά κανόνα πολύ μεγάλο) σύστημα. Η μεταφορά γίνεται διαμέσου των τιμών των βαρών που έχει προσδιορίσει μετά το πέρας της εκπαίδευσης.\n",
    "\n",
    "Στην περίπτωσή μας, δεν μας ενδιαφέρουν τόσο τα ίδια τα βάρη των μοντέλων από τα οποία θα κάνουμε μεταφορά μάθησης. Κάτι τέτοιο θα μας ενδιέφερε αν π.χ. θέλαμε να συνεχίσουμε την εκπαίδευση στα δικά μας κείμενα. Μας ενδιαφέρουν όμως τα ίδια τα εμφυτεύματα, δηλαδή τα embeddings (διανύσματα διαστάσεων $m$) που έχει μάθει το νευρωνικό για το λεξιλόγιο του (vocabulary). To vocabulary σε τέτοια μεγάλα νευρωνικά θα είναι πιθανότατα υπερσύνολο του δικού μας."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UFGxnb9iknm"
   },
   "source": [
    "### Μεταφορά μάθησης εμφυτευμάτων\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3Z28edwj4wF"
   },
   "source": [
    "#### Εμφυτεύματα του Gensim-data\n",
    "Το Gensim περιλαμβάνει αρκετά προεκπαιδευμένα μοντέλα εμφυτευμάτων Word2Vec. Με το επόμενο κελί παίρνουμε τη λίστα τους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "UX9ZkHSvi3Mi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U gensim\n",
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mS0XEVWUi_0e"
   },
   "source": [
    "Τα μοντέλα αυτά βρίσκονται στο [αποθετήριο Gensim-data](https://github.com/RaRe-Technologies/gensim-data) όπου μπορείτε να βρείτε και την τεκμηρίωσή τους. Η φόρτωση των μοντέλων αυτών γίνεται με τη συνάρτηση `gensim.downloader.load`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfNs5fMAkADF"
   },
   "source": [
    "#### Άλλα εμφυτεύμαατα\n",
    "Μπορείτε να βρείτε προεκπαιδευμένα εμφυτεύματα και από πηγές εκτός του Gensim. Για παράδειγμα:\n",
    "\n",
    "- [Google News dataset](https://code.google.com/archive/p/word2vec/). Πρόκειται για προ-εκπαιδευμένα διανύσματα που έχουν εκπαιδευτεί σε μέρος του συνόλου δεδομένων Google News (περίπου 100 δισεκατομμύρια λέξεις). Το μοντέλο περιέχει διανύσματα 300 διαστάσεων για 3 εκατομμύρια λέξεις και φράσεις.\n",
    "- [Amazon BlazingText](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html). Το BlazingText δεν είναι μόνο προεκπαιδευμένα εμφυτεύματα αλλα και βελτιστοποιημένες υλοποιήσεις των αλγορίθμων Word2vec για την επεξεργασία κειμένου. Προυπόθεση είναι να δουλέψει κανείς στο SageMaker.\n",
    "\n",
    "Οι διαδικασίες φόρτωσης embeddings από εξωτερικά δεδομένα μπορεί να είναι ελαφρά διαφορετικές από αυτή του Gensim.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkkZE41d_DjX"
   },
   "source": [
    "#### Παρατηρήσεις\n",
    "\n",
    "*   Επαναλαμβάνουμε ότι στην εργασία αυτή δεν μας ενδιαφέρουν τα ίδια τα μοντέλα αλλά το να μπορούμε για μία λέξη του λεξιλογίου μας να μπορούμε να βρούμε το embedding (διάνυσμα) που της αντιστοιχεί στο εκάστοτε προεκπαιδευμένο μοντέλο. \n",
    "\n",
    "*   Επίσης, δεν θα χρησιμοποιήσουμε την `Phrases` για να βρούμε bigrams στο dataset μας όπως θα ήταν το ορθότερο, καθώς αυτό θα απαιτούσε την συνέχιση της εκπαίδευσης του μοντέλου σε νέο λεξιλόγιο με πολύ λίγα νέα δεδομένα.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF6bQOziqISL"
   },
   "source": [
    " ### Δημιουργία corpora βασισμένων στα εμφυτεύματα\n",
    "\n",
    "Για να μπορέσουμε να ενσωματώσουμε τη γνώση που υπάρχει στα προεκπαιδευμένα εμφυτεύματα στο δικό μας corpus θα προχωρήσουμε όπως περιγράφεται ακολούθως.\n",
    "\n",
    "Για κάθε περιγραφή ταινίας $d$, η οποία αποτελείται από τις $N_d$ λέξεις $w_i$, το  $tfidf$ της κάθε λέξης $w_i$ δίνεται από τη σχέση:\n",
    "\n",
    "$$ tfidf(w_i) = tf(w_i,d) \\cdot idf(w_i)$$\n",
    "\n",
    "Ταυτόχρονα, σε κάθε λέξη $w_i$ αντιστοιχεί ένα διάνυσμα $W2V(w_i)$ από το μοντέλο εμφυτευμάτων που έχουμε εισάγει. Τα διανύσματα εμφυτευμάτων $W2V$ θα έχουν διάσταση $m$, ανάλογα το μοντέλο. \n",
    "\n",
    "Για κάθε ταινία d, μπορούμε να δημιουργήσουμε μια διανυσματική αναπαράσταση $W2V(d)$ διαστάσεων $m$ χρησιμοποιώντας το $tfidf(w_i)$ ως συντελεστή βαρύτητας για κάθε εμφύτευμα $W2V(w_i)$:\n",
    "\n",
    "$$ W2V(d) = \\frac{tfidf(w_1)\\cdot W2V(w_i) + tfidf(w_2)\\cdot W2V(w_2) + \\dotsc  + tfidf(w_{N_{d}})\\cdot W2V(w_{N_{d}})}{tfidf(w_1)+tfidf(w_2)+ \\dotsc + tfidf(w_{N_{d}})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Umbd7mv__be8"
   },
   "source": [
    "#### build_tfw2v\n",
    "\n",
    "Υλοποιήστε μια συνάρτηση `build_tfw2v` με ορίσματα:\n",
    "- `corpus` που θα είναι το προεπεξεργασμένο dataset σας,\n",
    "- `vectors` που θα είναι το μοντέλο που θα σας δίνει τα διανύσματα των εμφυτεύσεων vectors, και \n",
    "- `embeddings_size` που θα είναι η διάσταση των εμφυτευμάτων $m$.\n",
    "\n",
    "H συνάρτηση αυτή θα επιστρέφει ένα νέο corpus που θα είναι ένας πίνακας 5000 (όσες οι ταινίες σας) x $m$ (το η διάσταση των εμφυτευμάτων). Ανάλογα ποιο μοντέλο χρησιμποιείτε για transfer learning ο πίνακας αυτός θα είναι διαφορετικός.\n",
    "\n",
    "Μπορείτε πλεόν να καλείτε την `content_recommender` με διαφορετικά corpora στο όρισμα `corpus_type`. Σημειώστε ότι στο TFidfVectorizer χρησιμοποιουμε τη σειριακή μορφή των numpy arrays και ίσως σας χρησιμεύσει η `sparse.csr_matrix()` από την Scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model_google_news = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfw2v(corpus, vectors, embeddings_size):  \n",
    "    # initialize 2d array with the correct shape\n",
    "    arr_shape = (5000, embeddings_size)\n",
    "    tfw2v = np.zeros(arr_shape)\n",
    "    \n",
    "    # compute the w2v representation for each movie\n",
    "    row = 0\n",
    "    for movie_description in corpus:\n",
    "        \n",
    "        # initialize numerator and denominator of the w2v expression\n",
    "        numerator = np.zeros(embeddings_size)\n",
    "        denominator = 0\n",
    "        \n",
    "        # compute w2v of the movie considering every word in the movie description\n",
    "        for word in movie_description:\n",
    "            # if an embedding exists for the word do the computation normally\n",
    "            if word in vectors:\n",
    "                numerator += df_tf_idf[word][row] * vectors[word]\n",
    "                denominator += df_tf_idf[word][row]\n",
    "            # else do nothing\n",
    "\n",
    "            \n",
    "        # final computation \n",
    "        tfw2v[row] = numerator / denominator\n",
    "        row += 1\n",
    "    \n",
    "    return tfw2v\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfw2v_google_news = build_tfw2v(preprocessed_corpus, model_google_news, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|Movie|Title|description|categories|\n",
       "|:-:|:-:|:-:|:-:|\n",
       "|3291(target)|Cat Soup|nyāko, the older sister of nyatta, lies very ill in her room. by accident, nyatta drowns in the bathtub and while being clinically dead he sees his sister leaving the house holding hands with the japanese version of ksitigarbha,  and follows them. nyatta tries to claim his sister's soul back by pulling her arm. jizou doesn't let go and nyāko's soul gets split in two. her brother runs away with one half. jizou sends a clue about a flower they must search for in order to retrieve the missing part, then walks away with the other half. meanwhile, nyatta's father finds his son in the bathtub and revives him. then, all the family members gather in nyāko's room to find out that she is dead. nyatta gets closer with the half-soul in his arms and puts it back in his sister's body through her nose. nyāko wakes up braindead. after that nyatta and nyāko begin their journey . during their outing they visit the 'big whale circus', the final act of which is a giant penguin-looking bird \"filled\" with various weather, causes a flood of water which covers everything. the two of them end up on an ark-esque boat with a pig, which they eventually begin to eat . god drains the world of the oceans , leaving the cats and pig stranded in a desert. the pig bites off nyatta's arm, which is repaired by a desert-dweller who makes dolls from the pieces of other cats. traveling across the desert, they are brought to a house by the smell of food, and are invited inside by a man. they are fed, and when full the man attempts to turn them into soup, attacking them with a pair of scissors. he ends up falling into the cauldron, nyatta cuts him into pieces with the scissors and the cats escape. wandering further across the desert dehydrated, nyatta digs and finds an elephant made of water, which cools them off and travels with them, though the elephant eventually evaporates from the heat. god accidentally stops the flow of time and disrupts space, and the cats play with the time-frozen scenes. father time turns time back on, shooting it forward and reversing it, showing various scenes of random events either rapidly going forth in time or back. eventually the cats find themselves back on their boat in the ocean. they drift into a marsh of metallic plants and creatures, coming across the flower they were seeking. nyatta places the flower on nyākos face, which restores her to normal. together, they go back home. in the end, the entire family of cats are gathered in their house leisurely watching tv. nyatta leaves them to visit the toilet, and while he is gone, the other family members disappear one by one in thin air. the show on the tv also disappears, leaving only a flashing screen behind. nyatta returns, noticing that everyone is gone, and outside the nearby lamppost also turns off leaving the house in darkness. finally the movie also \"turns off\", leaving behind a flashing screen of static before the credits roll.|['\"Short Film\",  \"Anime\",  \"Comedy\",  \"Animation\"']|\n",
       "|490|Taxidermia|the film begins in a remote hungarian military outpost, where orderly morosgoványi vendel lives a wretched existence of servitude beneath the heel of his lieutenant, öreg balatony kálmán. condemned to performing menial duties for the officer and his family while sleeping in an unheated shack next to the latrines, morosgoványi frequently escapes into fantasy. so realistic are these fantasies that in one ambiguous instance, morosgoványi sleeps with and impregnates the lieutenant's wife and \"wakes up\" to find himself engaged in an act of sodomy with a slaughtered pig. upon seeing this, the lieutenant promptly executes morosgoványi and raises the son, balatony kálmán, as his own. decades later kálmán has grown into a champion hungarian speed eater. coached and influenced by the strict jenő, kálmán's life revolves around training for the eventual day when the ioc recognizes speed-eating as a legitimate sport. after a bout of lockjaw at a soviet event and eloping with fellow speed-eating champion aczél gizi, kálmán resumes his rigorous training, even as gizi gives birth to their son, balatony lajoska. decades later, lajoska has grown into a dedicated, professional taxidermist. in contrast to both his parents' girth, lajoska appears pale and impoverished, with a thin anemic frame and haunted visage. when not working from his taxidermy shop or failing in his attempts to lead a normal life, lajoska purchases groceries for his father kálmán, who has grown so monstrously obese that he cannot leave the chair in his claustrophobic apartment. kálmán, who feeds butter to his caged cats, has nothing but harsh words for his son who, upon reaching his breaking point, abandons his father to his own prison. returning later, he discovers that the cats have escaped their cages and, desiring meat, have eviscerated his father. lajoska stuffs his father and the cats. with little left to live for, he locks himself in a homemade surgical harness and through the use of sedatives and painkillers, begins removing his own internal organs. pumping his body full of preservatives and sewing himself up, he activates the machine that decapitates him, leaving behind a preserved statue. his body is displayed in an exhibit along with the cats and his father.|['\"Horror\",  \"Art film\",  \"World cinema\",  \"Surrealism\",  \"Gross-out film\",  \"Drama\",  \"Comedy\",  \"Gross out\",  \"Black comedy\"']|\n",
       "|2604|Amores perros|the film is constructed from three distinct stories linked by a car accident that brings the characters briefly together. ;octavio y susana the first segment stars gael garcía bernal and vanessa bauche as the title characters. susana is octavio's sister-in-law; however, octavio is in love with her and doesn't like the way his brother, ramiro, treats her. octavio tries to persuade her to run away with him to get from under ramiro's abuse. needing to make money so that he and susana can escape and start a life of their own, octavio becomes involved in the business of dog fighting. octavio makes enough money to run away with susana, but susana takes the money and leaves with her husband. octavio continues his dog fighting business until a rival owner shoots his dog cofi. octavio stabs the rival owner and finds himself in a car chase with his lifelong friend, jorge, and the wounded dog. a collision follows; jorge dies and octavio is badly injured. ;daniel y valeria the next segment stars álvaro guerrero and goya toledo. daniel is a successful magazine publisher who leaves his family to live with the spanish supermodel valeria played by goya toledo. valeria's leg is severely broken in the accident with octavio's car and she may be unable to continue working as a model. valeria is using a wheelchair while she recuperates in the apartment she shares with daniel. her dog richie disappears under the floorboards one day and stays there for days. the missing dog triggers serious tension for the couple, causing numerous fights which leads to doubts about their relationship on both sides. valeria re-injures her leg trying to help the dog, resulting in severe internal bleeding which leads to gangrene. her doctor is forced to amputate the leg, removing any chance she might have had at returning to her modeling career. once her leg is gone, she realizes that her life is most likely ruined since her sense of purpose, modeling, has been taken from her. ;el chivo y maru the final segment stars emilio echevarría and lourdes echevarría. the story concerns a former private school teacher who had become involved in guerrilla movements that landed him in prison for 20 years. he appears in the film as a bedraggled vagrant pushing a junk cart accompanied by several mongrel dogs for whom he cares. though he appears to live in perpetual squalor in an abandoned warehouse, he is in fact a professional hitman, el chivo . at times throughout his story, chivo tries to make contact with his daughter, maru, whom he abandoned when she was a two-year-old child when he began his guerrilla involvement. her mother told her that her father had died, instead of telling maru the truth about the abandonment and the prison sentence. meanwhile, ramiro and an accomplice are attempting to rob a bank when he is shot and killed. octavio, seriously injured from the accident, sees susana for the first time since she and ramiro fled with his money. despite having been wronged, octavio tries again to get susana to run away with him, but she becomes angry with the fact that octavio is willing to run away with her after she has just lost somebody she loves. el chivo is hired by a man to kill his business partner, and chivo is about to make the kill when the film's central car crash interrupts him. during the chaos at the crash scene, chivo steals octavio's wounded dog and takes it home to nurture it. while chivo is away from the warehouse one day, the rescued dog kills all of the other dogs in the house due to its previous dog fighting. chivo is intensely upset and prepares to kill the dog but forgives him as he knows no better. still grieving for his beloved dogs, chivo captures his intended victim, and after learning that the victim is the client's half-brother, he also captures his client. after shaving his beard and grooming his hair  he leaves both men alive and chained to the separate walls with a pistol within reach between them, their fate left undetermined. he then breaks into his daughter's house while she is away, he leaves her a large bundle of money and leaves a message on her answering machine explaining what happened to him and why the family was split as well as telling her he loves her, but just as before he says it, the answer machine stops recording so it doesn't record when he tells her how much he loves her. then he goes to an autoshop where he sells the client's suv. the mechanic asks him what is the name of the dog; he calls him \"negro\" . after he receives the money for the car chivo and negro walk away and disappear once more.|['\"Thriller\",  \"Ensemble Film\",  \"Drama\"']|\n",
       "|3056|The Land Before Time|in a time overlapping the jurassic period and cretaceous period, a drought is occurring and several herds of dinosaurs seek an oasis known as the \"great valley\". among these, a diminished \"longneck\" herd gives birth to a single baby, named littlefoot . years later, littlefoot plays with cera , a \"three-horn\", who was trying to smash a beetle until her father  intervenes; whereupon littlefoot's mother  names the different kinds of dinosaurs: \"three-horns\", \"spiketails\", \"big mouths\", and \"flyers\" and states that each has historically remained apart. that night, as littlefoot follows a \"hopper\", he encounters cera again, and they play together briefly until a \"sharptooth\" attacks. he almost has them, before littlefoot's mother comes to their rescue. during their escape, she suffers severe back and neck injuries from the sharptooth's teeth and claws. at that same time, an \"earthshake\" opens a deep ravine that swallows up the sharptooth and divides littlefoot and cera from their herds. littlefoot finds his dying mother, and receives her advice in favor of his intuition. depressed and confused, littlefoot meets an old scolosaurus named rooter , who consoles him upon learning of his mother's death. littlefoot later hears his mother's voice guiding him to follow the \"bright circle\" past the \"great rock that looks like a longneck\" and then past the \"mountains that burn\" to the great valley. on his journey , littlefoot meets cera once again and tries to get her to join him, but she refuses. later, littlefoot is accompanied by a young \"bigmouth/swimmer\" named ducky , whose company bears him out of his depression. soon after, they meet an aerophobic \"flyer\" named petrie . cera, who is attempting to find her own kind, finds the unconscious sharptooth inside the ravine. thinking he is dead, cera harasses him, during which she mistakenly wakes him up, and flees. she later bumps into littlefoot, ducky, and petrie, and tells them that the sharptooth is alive; although littlefoot does not believe her. she then describes her encounter , during which she accidentally flings ducky into the air and discovers a hatchling \"spiketail\", whom she names spike and brings him into the group. seeking the great valley, they discover a cluster of trees, which is abruptly depleted by a herd of diplodocus. searching for remaining growth, they discover one tree still with leaves, and obtain food by stacking up atop each other and pulling the leaves down. cera remains aloof; but at nightfall, everyone including herself gravitates to littlefoot's side for warmth and companionship. the next morning, they are attacked by sharptooth, but escape through a cave-tunnel too small to admit him. beyond this, they discover the longneck-shaped monolith mentioned by littlefoot's mother, and later a string of \"mountains that burn\". cera grows impatient of the seemingly resultless trip and decides to go another way, but littlefoot refuses, telling her the way she is going is wrong and when cera refuses to retract an insult about littlefoot's mother , a fight between the two ensues causing a schism in the travelling party whereby littlefoot continues in the direction he was told, while the others follow cera. when ducky and spike become endangered by lava and petrie gets stuck in a tar pit, littlefoot rescues them; later to find cera harassed by a small territorial herd of \"boneheads\", and, having been coated in tar, scare them away. ashamed of her fear and reluctant to admit her mistake, cera leaves them in tears. later, while crossing a pond, petrie discovers the sharptooth nearby. tired of the sharptooth stalking them and determined to avenge his mother, littlefoot plots to lure him into the water beneath a nearby boulder, intending to drown him. as ducky  lures sharptooth to the water, littlefoot and spike are having trouble moving the boulder. during the proceeding struggle, a draft from sharptooth's nostrils enables petrie to flight. sharptooth leaps onto the boulder and the plan nearly fails until cera reunites with the group, allowing littlefoot and his friends to push both sharptooth, petrie and the boulder into the water below, momentarily taking petrie down with him; but he later emerges unharmed. littlefoot, alone, follows a cloud resembling his mother to the great valley, there to be joined by the others. upon arrival, petrie impresses his family with his newfound flight, while ducky introduces spike to her family, who adopt him. cera reunites with her father and littlefoot rejoins his grandparents. cera then calls for littlefoot to play. they join their friends at the top of a hill and embrace each other in a group hug.|['\"Adventure\",  \"Children\\'s/Family\",  \"Costume drama\",  \"Animation\",  \"Children\\'s\",  \"Fantasy\",  \"Comedy\",  \"Coming of age\",  \"Family Film\",  \"Children\\'s Fantasy\"']|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "r = random.randint(1,5000)\n",
    "content_recommender(r,3,sparse.csr_matrix(tfw2v_google_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPVK7Z5c1p5F"
   },
   "source": [
    "## Ανάλυση αποτελεσμάτων\n",
    "\n",
    "### Σύστημα συστάσεων βασισμένο μόνο στο tfidf\n",
    "\n",
    "- Σε markdown περιγράψτε τι προεπεξεργασία κάνετε στα κείμενα και γιατί.\n",
    "\n",
    "- Περιγράψτε πως προχωρήσατε στις επιλογές σας για τη βελτιστοποίηση της `TfidfVectorizer`. \n",
    "\n",
    "- [Cherry-picking:](https://www.wikiwand.com/en/Cherry_picking) Δώσετε παραδείγματα (IDs) από τη συλλογή σας που επιστρέφουν καλά αποτελέσματα μέχρι `max_recommendations` (τουλάχιστον 5) και σχολιάστε.\n",
    "\n",
    "- [Nit-picking:](https://en.wikipedia.org/wiki/Nitpicking) Δώστε παραδείγματα (IDs) από τη συλλογή σας που επιστρέφουν κακά αποτελέσματα και σχολιάστε.\n",
    "\n",
    "- Ποια είναι συνολικά τα πλεονεκτήματα και μειονεκτήματα ενός recommender βασισμένου στο tfidf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPBW9WH56I-y"
   },
   "source": [
    "### Σύγκριση και σχολιασμός με recommenders βασισμένων στο Word2Vec\n",
    "\n",
    "- Υλoποιήστε recommenders που βασίζονται σε μεταφορά μάθησης και εμφυτεύματα. Χρησιμοποιήστε παραδείγματα για να υποδείξετε δυνατά και αδύναμα σημεία τους.\n",
    "\n",
    "- Μπορείτε να σχολιάσετε τα recommenders που βασίζονται στο Word2Vec σε σχέση με το απλό μοντέλο tfidf, εξετάζοντας τις συστάσεις για ίδια ID.\n",
    "\n",
    "- Μπορείτε επίσης να εξετάσετε συγκριτικά τα Word2Vec recommenders μεταξύ τους και πάλι βασιζόμενοι σε παραδείγματα.\n",
    "\n",
    "- Οι παρατηρήσεις σας θα βασίζονται στην ανάλυση των ποιοτικών χαρακτηριστικών που είναι η σειρά και το σύνολο των συστάσεων. Ωστόσο, μπορείτε να συμπεριλάβετε και ποσοτικά χαρακτηριστικά όπως τους χρονους loading και συγκρότησης του corpus αλλά και της διαστατικότητας $m$.\n",
    "\n",
    "Χρησιμοποιήστε όποια μορφή reporting κρίνετε καταλληλότερη: κείμενο, πίνακες, διαγράμματα.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4irg4K-IPSym"
   },
   "source": [
    "## Πρακτικό tip - persistence αντικειμένων με joblib.dump\n",
    "\n",
    "Καθώς στην δεύτερη εργασία καλείστε να δημιουργήσετε διάφορα corpora των οποίων η δημιουργία παίρνει χρόνο, υπάρχει ένας εύκολος τρόπος να αποθηκεύουμε μεταβλητές σε dump files και να τις διαβάζουμε απευθείας.\n",
    "\n",
    "H βιβλιοθήκη [joblib](https://pypi.python.org/pypi/joblib) της Python δίνει κάποιες εξαιρετικά χρήσιμες ιδιότητες στην ανάπτυξη κώδικα: pipelining, παραλληλισμό, caching και variable persistence. Τις τρεις πρώτες ιδιότητες τις είδαμε στην πρώτη άσκηση. Στην παρούσα άσκηση θα μας φανεί χρήσιμη η τέταρτη, το persistence των αντικειμένων. Συγκεκριμένα μπορούμε με:\n",
    "\n",
    "```python\n",
    "joblib.dump(my_object, 'my_object.pkl') \n",
    "```\n",
    "\n",
    "να αποθηκεύσουμε οποιοδήποτε αντικείμενο-μεταβλητή (εδώ το `my_object`) απευθείας πάνω στο filesystem ως αρχείο, το οποίο στη συνέχεια μπορούμε να ανακαλέσουμε ως εξής:\n",
    "\n",
    "```python\n",
    "my_object = joblib.load('my_object.pkl')\n",
    "```\n",
    "\n",
    "Μπορούμε έτσι να ανακαλέσουμε μεταβλητές ακόμα και αφού κλείσουμε και ξανανοίξουμε το notebook, χωρίς να χρειαστεί να ακολουθήσουμε ξανά όλα τα βήματα ένα - ένα για την παραγωγή τους, κάτι ιδιαίτερα χρήσιμο αν αυτή η διαδικασία είναι χρονοβόρα.\n",
    "\n",
    "Ας αποθηκεύσουμε το `corpus_tf_idf` και στη συνέχεια ας το ανακαλέσουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aESOPYQaPSyo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(corpus_tf_idf, 'corpus_tf_idf.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_rAEj5ZPSy1"
   },
   "source": [
    "\n",
    "\n",
    "Μπορείτε με ένα απλό `!ls` να δείτε ότι το αρχείο `corpus_tf_idf.pkl` υπάρχει στο filesystem σας (== persistence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhwXmTEIPSy3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cey5AbkO475S"
   },
   "source": [
    "και μπορούμε να τα διαβάσουμε με `joblib.load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSJPTKY8PSyu"
   },
   "outputs": [],
   "source": [
    "corpus_tf_idf = joblib.load('corpus_tf_idf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHOQtO83PSy9"
   },
   "source": [
    "# Εφαρμογή 2.  Τοπολογική και σημασιολογική απεικόνιση της ταινιών με χρήση SOM\n",
    "<img src=\"https://i.imgur.com/Z4FdurD.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UB_clmizPSy-"
   },
   "source": [
    "## Δημιουργία dataset\n",
    "Στη δεύτερη εφαρμογή θα βασιστούμε στις τοπολογικές ιδιότητες των Self Organizing Maps (SOM) για να φτιάξουμε ενά χάρτη (grid) δύο διαστάσεων όπου θα απεικονίζονται όλες οι ταινίες της συλλογής της ομάδας με τρόπο χωρικά συνεκτικό ως προς το περιεχόμενο και κυρίως το είδος τους (ο παραπάνω χάρτης είναι ενδεικτικός, δεν αντιστοιχεί στο dataset μας). \n",
    "\n",
    "Διαλέξτε για την αναπαράσταση των documents αυτήν που πιστεύετε απέδωσε καλύτερα στο πρώτα σκέλος της άσκησης. Έστω ότι αυτή είναι η `my_best_corpus`.\n",
    "\n",
    "Η έτοιμη συνάρτηση `build_final_set` θα ενώσει την αναπαράσταση που θα της δώσετε ως όρισμα `mycorpus` με τις binarized κατηγορίες `catbins` των ταινιών ως επιπλέον κολόνες (χαρακτηριστικά). Συνεπώς, κάθε ταινία αναπαρίσταται στο Vector Space Model από τα χαρακτηριστικά της αναπαράστασης `mycorpus` και τις κατηγορίες της.\n",
    "\n",
    "Τέλος, η συνάρτηση δέχεται ένα ορισμα για το πόσες ταινίες να επιστρέψει, με default τιμή όλες τις ταινίες (5000). Αυτό είναι χρήσιμο για να μπορείτε αν θέλετε να φτιάχνετε μικρότερα σύνολα δεδομένων ώστε να εκπαιδεύεται ταχύτερα το SOM. \n",
    "\n",
    "Θα τρέχουμε τη συνάρτηση με `final_set = build_final_set(my_best_corpus)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-FDDOkQPSzA"
   },
   "outputs": [],
   "source": [
    "def build_final_set(mycorpus, doc_limit = 5000, tf_idf_only=False):\n",
    "    # convert sparse tf_idf to dense tf_idf representation\n",
    "    dense_tf_idf = mycorpus.toarray()[0:doc_limit,:]\n",
    "    if tf_idf_only:\n",
    "        # use only tf_idf\n",
    "        final_set = dense_tf_idf\n",
    "    else:\n",
    "        # append the binary categories features horizontaly to the (dense) tf_idf features\n",
    "        final_set = np.hstack((dense_tf_idf, catbins[0:doc_limit,:]))\n",
    "    # η somoclu θέλει δεδομ΄ένα σε float32\n",
    "    return np.array(final_set, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjvPPENS_dYL"
   },
   "source": [
    "Στο επόμενο κελί, τυπώνουμε τις διαστάσεις του τελικού dataset μας. **Χωρίς βελτιστοποίηση του TFIDF** θα έχουμε περίπου 50.000 χαρακτηριστικά και ο θα είναι ανέφικτο να προχωρήσουμε στην εκπαίδευση του SOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvEgNn-L-jEw"
   },
   "outputs": [],
   "source": [
    "final_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tikdip0PSzQ"
   },
   "source": [
    "## Εκπαίδευση χάρτη SOM\n",
    "\n",
    "Θα δουλέψουμε με τη βιβλιοθήκη SOM [\"Somoclu\"](http://somoclu.readthedocs.io/en/stable/index.html). Εισάγουμε τις somoclu και matplotlib και λέμε στη matplotlib να τυπώνει εντός του notebook (κι όχι σε pop up window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oX9rzxGSPSzR"
   },
   "outputs": [],
   "source": [
    "# install somoclu\n",
    "!pip install --upgrade somoclu\n",
    "# import sompoclu, matplotlib\n",
    "import somoclu\n",
    "import matplotlib\n",
    "# we will plot inside the notebook and not in separate window\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqBfn0ijPSzX"
   },
   "source": [
    "Καταρχάς διαβάστε το [function reference](http://somoclu.readthedocs.io/en/stable/reference.html) του somoclu. Θα δoυλέψουμε με χάρτη τύπου planar, παραλληλόγραμμου σχήματος νευρώνων με τυχαία αρχικοποίηση (όλα αυτά είναι default). Μπορείτε να δοκιμάσετε διάφορα μεγέθη χάρτη ωστόσο όσο ο αριθμός των νευρώνων μεγαλώνει, μεγαλώνει και ο χρόνος εκπαίδευσης. Για το training δεν χρειάζεται να ξεπεράσετε τα 100 epochs. Σε γενικές γραμμές μπορούμε να βασιστούμε στις default παραμέτρους μέχρι να έχουμε τη δυνατότητα να οπτικοποιήσουμε και να αναλύσουμε ποιοτικά τα αποτελέσματα. Ξεκινήστε με ένα χάρτη 10 x 10, 100 epochs training και ένα υποσύνολο των ταινιών (π.χ. 2000). Χρησιμοποιήστε την `time` για να έχετε μια εικόνα των χρόνων εκπαίδευσης. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntd2GE9SaHiS"
   },
   "source": [
    "\n",
    "## Best matching units\n",
    "\n",
    "Μετά από κάθε εκπαίδευση αποθηκεύστε σε μια μεταβλητή τα best matching units (bmus) για κάθε ταινία. Τα bmus μας δείχνουν σε ποιο νευρώνα ανήκει η κάθε ταινία. **Προσοχή: η σύμβαση των συντεταγμένων των νευρώνων στη Somoclu είναι (στήλη, γραμμή) δηλαδή το ανάποδο από την Python**. Με χρήση της [np.unique](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html) (μια πολύ χρήσιμη συνάρτηση στην άσκηση) αποθηκεύστε τα μοναδικά best matching units και τους δείκτες τους (indices) προς τις ταινίες. \n",
    "\n",
    "Σημειώστε ότι μπορεί να έχετε λιγότερα μοναδικά bmus από αριθμό νευρώνων γιατί μπορεί σε κάποιους νευρώνες να μην έχουν ανατεθεί ταινίες. Ως αριθμό νευρώνα θα θεωρήσουμε τον αριθμό γραμμής στον πίνακα μοναδικών bmus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grzqcyHyaKdg"
   },
   "source": [
    "\n",
    "## Ομαδοποίηση (clustering)\n",
    "\n",
    "Τυπικά, η ομαδοποίηση σε ένα χάρτη SOM προκύπτει από το unified distance matrix (U-matrix): για κάθε κόμβο υπολογίζεται η μέση απόστασή του από τους γειτονικούς κόμβους. Εάν χρησιμοποιηθεί μπλε χρώμα στις περιοχές του χάρτη όπου η τιμή αυτή είναι χαμηλή (μικρή απόσταση) και κόκκινο εκεί που η τιμή είναι υψηλή (μεγάλη απόσταση), τότε μπορούμε να πούμε ότι οι μπλε περιοχές αποτελούν clusters και οι κόκκινες αποτελούν σύνορα μεταξύ clusters.\n",
    "\n",
    "To somoclu δίνει την επιπρόσθετη δυνατότητα να κάνουμε ομαδοποίηση των νευρώνων χρησιμοποιώντας οποιονδήποτε αλγόριθμο ομαδοποίησης του scikit-learn. Στην άσκηση θα χρησιμοποιήσουμε τον k-Means. Για τον αρχικό σας χάρτη δοκιμάστε ένα k=20 ή 25. Οι δύο προσεγγίσεις ομαδοποίησης είναι διαφορετικές, οπότε περιμένουμε τα αποτελέσματα να είναι κοντά αλλά όχι τα ίδια.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nupuqcuaMe3"
   },
   "source": [
    "\n",
    "## Αποθήκευση του SOM\n",
    "\n",
    "Επειδή η αρχικοποίηση του SOM γίνεται τυχαία και το clustering είναι και αυτό στοχαστική διαδικασία, οι θέσεις και οι ετικέτες των νευρώνων και των clusters θα είναι διαφορετικές κάθε φορά που τρέχετε τον χάρτη, ακόμα και με τις ίδιες παραμέτρους. Για να αποθηκεύσετε ένα συγκεκριμένο som και clustering χρησιμοποιήστε και πάλι την `joblib`. Μετά την ανάκληση ενός SOM θυμηθείτε να ακολουθήσετε τη διαδικασία για τα bmus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejX0Qs18aRHU"
   },
   "source": [
    "\n",
    "## Οπτικοποίηση U-matrix, clustering και μέγεθος clusters\n",
    "\n",
    "Για την εκτύπωση του U-matrix χρησιμοποιήστε τη `view_umatrix` με ορίσματα `bestmatches=True` και `figsize=(15, 15)` ή `figsize=(20, 20)`. Τα διαφορετικά χρώματα που εμφανίζονται στους κόμβους αντιπροσωπεύουν τα διαφορετικά clusters που προκύπτουν από τον k-Means. Μπορείτε να εμφανίσετε τη λεζάντα του U-matrix με το όρισμα `colorbar`. Μην τυπώνετε τις ετικέτες (labels) των δειγμάτων, είναι πολύ μεγάλος ο αριθμός τους.\n",
    "\n",
    "Για μια δεύτερη πιο ξεκάθαρη οπτικοποίηση του clustering τυπώστε απευθείας τη μεταβλητή `clusters`.\n",
    "\n",
    "Τέλος, χρησιμοποιώντας πάλι την `np.unique` (με διαφορετικό όρισμα) και την `np.argsort` (υπάρχουν και άλλοι τρόποι υλοποίησης) εκτυπώστε τις ετικέτες των clusters (αριθμοί από 0 έως k-1) και τον αριθμό των νευρώνων σε κάθε cluster, με φθίνουσα ή αύξουσα σειρά ως προς τον αριθμό των νευρώνων. Ουσιαστικά είναι ένα εργαλείο για να βρίσκετε εύκολα τα μεγάλα και μικρά clusters. \n",
    "\n",
    "Ακολουθεί ένα μη βελτιστοποιημένο παράδειγμα για τις τρεις προηγούμενες εξόδους:\n",
    "\n",
    "<img src=\"https://image.ibb.co/i0tsfR/umatrix_s.jpg\" width=\"35%\">\n",
    "<img src=\"https://image.ibb.co/nLgHEm/clusters.png\" width=\"35%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMO_KcQYaTv-"
   },
   "source": [
    "\n",
    "## Σημασιολογική ερμηνεία των clusters\n",
    "\n",
    "Προκειμένου να μελετήσουμε τις τοπολογικές ιδιότητες του SOM και το αν έχουν ενσωματώσει σημασιολογική πληροφορία για τις ταινίες διαμέσου της διανυσματικής αναπαράστασης του tf-idf, των εμφυτευμάτων και των κατηγοριών, χρειαζόμαστε ένα κριτήριο ποιοτικής επισκόπησης των clusters. \n",
    "\n",
    "Θα υλοποιήσουμε το εξής κριτήριο: Λαμβάνουμε όρισμα έναν αριθμό (ετικέτα) cluster. Για το cluster αυτό βρίσκουμε όλους τους νευρώνες που του έχουν ανατεθεί από τον k-Means. Για όλους τους νευρώνες αυτούς βρίσκουμε όλες τις ταινίες που τους έχουν ανατεθεί (για τις οποίες αποτελούν bmus). Για όλες αυτές τις ταινίες τυπώνουμε ταξινομημένη τη συνολική στατιστική όλων των ειδών (κατηγοριών) και τις συχνότητές τους. Αν το cluster διαθέτει καλή συνοχή και εξειδίκευση, θα πρέπει κάποιες κατηγορίες να έχουν σαφώς μεγαλύτερη συχνότητα από τις υπόλοιπες. Θα μπορούμε τότε να αναθέσουμε αυτήν/ές την/τις κατηγορία/ες ως ετικέτες κινηματογραφικού είδους στο cluster.\n",
    "\n",
    "Μπορείτε να υλοποιήσετε τη συνάρτηση αυτή όπως θέλετε. Μια πιθανή διαδικασία θα μπορούσε να είναι η ακόλουθη:\n",
    "\n",
    "1. Ορίζουμε συνάρτηση `print_categories_stats` που δέχεται ως είσοδο λίστα με ids ταινιών. Δημιουργούμε μια κενή λίστα συνολικών κατηγοριών. Στη συνέχεια, για κάθε ταινία επεξεργαζόμαστε το string `categories` ως εξής: δημιουργούμε μια λίστα διαχωρίζοντας το string κατάλληλα με την `split` και αφαιρούμε τα whitespaces μεταξύ ετικετών με την `strip`. Προσθέτουμε τη λίστα αυτή στη συνολική λίστα κατηγοριών με την `extend`. Τέλος χρησιμοποιούμε πάλι την `np.unique` για να μετρήσουμε συχνότητα μοναδικών ετικετών κατηγοριών και ταξινομούμε με την `np.argsort`. Τυπώνουμε τις κατηγορίες και τις συχνότητες εμφάνισης ταξινομημένα. Χρήσιμες μπορεί να σας φανούν και οι `np.ravel`, `np.nditer`, `np.array2string` και `zip`.\n",
    "\n",
    "2. Ορίζουμε τη βασική μας συνάρτηση `print_cluster_neurons_movies_report` που δέχεται ως όρισμα τον αριθμό ενός cluster. Με τη χρήση της `np.where` μπορούμε να βρούμε τις συντεταγμένες των bmus που αντιστοιχούν στο cluster και με την `column_stack` να φτιάξουμε έναν πίνακα bmus για το cluster. Προσοχή στη σειρά (στήλη - σειρά) στον πίνακα bmus. Για κάθε bmu αυτού του πίνακα ελέγχουμε αν υπάρχει στον πίνακα μοναδικών bmus που έχουμε υπολογίσει στην αρχή συνολικά και αν ναι προσθέτουμε το αντίστοιχο index του νευρώνα σε μια λίστα. Χρήσιμες μπορεί να είναι και οι `np.rollaxis`, `np.append`, `np.asscalar`. Επίσης πιθανώς να πρέπει να υλοποιήσετε ένα κριτήριο ομοιότητας μεταξύ ενός bmu και ενός μοναδικού bmu από τον αρχικό πίνακα bmus.\n",
    "\n",
    "3. Υλοποιούμε μια βοηθητική συνάρτηση `neuron_movies_report`. Λαμβάνει ένα σύνολο νευρώνων από την `print_cluster_neurons_movies_report` και μέσω της `indices` φτιάχνει μια λίστα με το σύνολο ταινιών που ανήκουν σε αυτούς τους νευρώνες. Στο τέλος καλεί με αυτή τη λίστα την `print_categories_stats` που τυπώνει τις στατιστικές των κατηγοριών.\n",
    "\n",
    "Μπορείτε βέβαια να προσθέσετε οποιαδήποτε επιπλέον έξοδο σας βοηθάει. Μια χρήσιμη έξοδος είναι πόσοι νευρώνες ανήκουν στο cluster και σε πόσους και ποιους από αυτούς έχουν ανατεθεί ταινίες.\n",
    "\n",
    "Θα επιτελούμε τη σημασιολογική ερμηνεία του χάρτη καλώντας την `print_cluster_neurons_movies_report` με τον αριθμός ενός cluster που μας ενδιαφέρει. \n",
    "\n",
    "Παράδειγμα εξόδου για ένα cluster (μη βελτιστοποιημένος χάρτης, ωστόσο βλέπετε ότι οι μεγάλες κατηγορίες έχουν σημασιολογική  συνάφεια):\n",
    "\n",
    "```\n",
    "Overall Cluster Genres stats:  \n",
    "[('\"Horror\"', 86), ('\"Science Fiction\"', 24), ('\"B-movie\"', 16), ('\"Monster movie\"', 10), ('\"Creature Film\"', 10), ('\"Indie\"', 9), ('\"Zombie Film\"', 9), ('\"Slasher\"', 8), ('\"World cinema\"', 8), ('\"Sci-Fi Horror\"', 7), ('\"Natural horror films\"', 6), ('\"Supernatural\"', 6), ('\"Thriller\"', 6), ('\"Cult\"', 5), ('\"Black-and-white\"', 5), ('\"Japanese Movies\"', 4), ('\"Short Film\"', 3), ('\"Drama\"', 3), ('\"Psychological thriller\"', 3), ('\"Crime Fiction\"', 3), ('\"Monster\"', 3), ('\"Comedy\"', 2), ('\"Western\"', 2), ('\"Horror Comedy\"', 2), ('\"Archaeology\"', 2), ('\"Alien Film\"', 2), ('\"Teen\"', 2), ('\"Mystery\"', 2), ('\"Adventure\"', 2), ('\"Comedy film\"', 2), ('\"Combat Films\"', 1), ('\"Chinese Movies\"', 1), ('\"Action/Adventure\"', 1), ('\"Gothic Film\"', 1), ('\"Costume drama\"', 1), ('\"Disaster\"', 1), ('\"Docudrama\"', 1), ('\"Film adaptation\"', 1), ('\"Film noir\"', 1), ('\"Parody\"', 1), ('\"Period piece\"', 1), ('\"Action\"', 1)]```\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq4QrImhaa7E"
   },
   "source": [
    "\n",
    "## Tips για το SOM και το clustering\n",
    "\n",
    "- Για την ομαδοποίηση ένα U-matrix καλό είναι να εμφανίζει και μπλε-πράσινες περιοχές (clusters) και κόκκινες περιοχές (ορίων). Παρατηρήστε ποια σχέση υπάρχει μεταξύ αριθμού ταινιών στο final set, μεγέθους grid και ποιότητας U-matrix.\n",
    "- Για το k του k-Means προσπαθήστε να προσεγγίζει σχετικά τα clusters του U-matrix (όπως είπαμε είναι διαφορετικοί μέθοδοι clustering). Μικρός αριθμός k δεν θα σέβεται τα όρια. Μεγάλος αριθμός θα δημιουργεί υπο-clusters εντός των clusters που φαίνονται στο U-matrix. Το τελευταίο δεν είναι απαραίτητα κακό, αλλά μεγαλώνει τον αριθμό clusters που πρέπει να αναλυθούν σημασιολογικά.\n",
    "- Σε μικρούς χάρτες και με μικρά final sets δοκιμάστε διαφορετικές παραμέτρους για την εκπαίδευση του SOM. Σημειώστε τυχόν παραμέτρους που επηρεάζουν την ποιότητα του clustering για το dataset σας ώστε να τις εφαρμόσετε στους μεγάλους χάρτες.\n",
    "- Κάποια τοπολογικά χαρακτηριστικά εμφανίζονται ήδη σε μικρούς χάρτες. Κάποια άλλα χρειάζονται μεγαλύτερους χάρτες. Δοκιμάστε μεγέθη 20x20, 25x25 ή και 30x30 και αντίστοιχη προσαρμογή των k. Όσο μεγαλώνουν οι χάρτες, μεγαλώνει η ανάλυση του χάρτη αλλά μεγαλώνει και ο αριθμός clusters που πρέπει να αναλυθούν.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4IUl8O8ayVf"
   },
   "source": [
    "\n",
    "\n",
    "## Ανάλυση τοπολογικών ιδιοτήτων χάρτη SOM\n",
    "\n",
    "Μετά το πέρας της εκπαίδευσης και του clustering θα έχετε ένα χάρτη με τοπολογικές ιδιότητες ως προς τα είδη των ταίνιών της συλλογής σας, κάτι αντίστοιχο με την εικόνα στην αρχή της Εφαρμογής 2 αυτού του notebook. Η συγκεκριμένη εικόνα είναι μόνο για εικονογράφιση, δεν είναι χάρτης SOM καιδεν έχει καμία σχέση με τη συλλογή δεδομένων και τις κατηγορίες μας.\n",
    "\n",
    "Για τον τελικό χάρτη SOM που θα παράξετε για τη συλλογή σας, αναλύστε σε markdown με συγκεκριμένη αναφορά σε αριθμούς clusters και τη σημασιολογική ερμηνεία τους τις εξής τρεις τοπολογικές ιδιότητες του SOM: \n",
    "\n",
    "1. Δεδομένα που έχουν μεγαλύτερη πυκνότητα πιθανότητας στο χώρο εισόδου τείνουν να απεικονίζονται με περισσότερους νευρώνες στο χώρο μειωμένης διαστατικότητας. Δώστε παραδείγματα από συχνές και λιγότερο συχνές κατηγορίες ταινιών. Χρησιμοποιήστε τις στατιστικές των κατηγοριών στη συλλογή σας και τον αριθμό κόμβων που χαρακτηρίζουν.\n",
    "2. Μακρινά πρότυπα εισόδου τείνουν να απεικονίζονται απομακρυσμένα στο χάρτη. Υπάρχουν χαρακτηριστικές κατηγορίες ταινιών που ήδη από μικρούς χάρτες τείνουν να τοποθετούνται σε διαφορετικά ή απομονωμένα σημεία του χάρτη.\n",
    "3. Κοντινά πρότυπα εισόδου τείνουν να απεικονίζονται κοντά στο χάρτη. Σε μεγάλους χάρτες εντοπίστε είδη ταινιών και κοντινά τους υποείδη.\n",
    "\n",
    "Προφανώς τοποθέτηση σε 2 διαστάσεις που να σέβεται μια απόλυτη τοπολογία δεν είναι εφικτή, αφενός γιατί δεν υπάρχει κάποια απόλυτη εξ ορισμού για τα κινηματογραφικά είδη ακόμα και σε πολλές διαστάσεις, αφετέρου γιατί πραγματοποιούμε μείωση διαστατικότητας.\n",
    "\n",
    "Εντοπίστε μεγάλα clusters και μικρά clusters που δεν έχουν σαφή χαρακτηριστικά. Εντοπίστε clusters συγκεκριμένων ειδών που μοιάζουν να μην έχουν τοπολογική συνάφεια με γύρω περιοχές. Προτείνετε πιθανές ερμηνείες.\n",
    "\n",
    "\n",
    "Τέλος, εντοπίστε clusters που έχουν κατά την άποψή σας ιδιαίτερο ενδιαφέρον στη συλλογή της ομάδας σας (data exploration / discovery value) και σχολιάστε.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYjxGR5DawIy"
   },
   "source": [
    "\n",
    "# Τελική παράδοση άσκησης\n",
    "\n",
    "- Θα παραδώσετε στο helios το παρόν notebook επεξεργασμένο ή ένα ή δύο νέα zipαρισμένα με τις απαντήσεις σας για τα ζητούμενα και των δύο εφαρμογών. \n",
    "- Θυμηθείτε ότι η ανάλυση του χάρτη στο markdown με αναφορά σε αριθμούς clusters πρέπει να αναφέρεται στον τελικό χάρτη με τα κελιά ορατά που θα παραδώσετε αλλιώς ο χάρτης που θα προκύψει θα είναι διαφορετικός και τα labels των clusters δεν θα αντιστοιχούν στην ανάλυσή σας. \n",
    "- Μην ξεχάσετε στην αρχή ένα κελί markdown με **τα στοιχεία της ομάδας σας**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHhCkvxjnitd"
   },
   "source": [
    "<table>\n",
    "  <tr><td align=\"center\">\n",
    "    <font size=\"4\">Παρακαλούμε διατρέξτε βήμα-βήμα το notebook για να μην ξεχάσετε παραδοτέα</font>\n",
    "</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
